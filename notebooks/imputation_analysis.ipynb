{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe0a116",
   "metadata": {},
   "source": [
    "# Survey-to-survey Imputation Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "215f4406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5333901",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = \"C:\\\\Users\\\\md82\\\\OneDrive - Anglia Ruskin University\\\\Documents\\\\mj-datalab\\\\poverty-prediction-challenge\"\n",
    "DATA_DIR = MAIN_DIR + \"\\\\data\\\\raw\"\n",
    "SUB_DIR = MAIN_DIR + \"\\\\data\\\\submission\"\n",
    "TRAIN_DATA = DATA_DIR + \"\\\\train_hh_features.csv\"\n",
    "TEST_DATA = DATA_DIR + \"\\\\test_hh_features.csv\"\n",
    "HH_DATA = DATA_DIR + \"\\\\train_hh_gt.csv\"\n",
    "PR_DATA = DATA_DIR + \"\\\\train_rates_gt.csv\"\n",
    "\n",
    "train_data = pd.read_csv(TRAIN_DATA)\n",
    "test_data = pd.read_csv(TEST_DATA)\n",
    "hh_data = pd.read_csv(HH_DATA)  \n",
    "pr_data = pd.read_csv(PR_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "999b0fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (104234, 88)\n",
      "Rows: 104234\n",
      "Columns: 88\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hhid</th>\n",
       "      <th>com</th>\n",
       "      <th>weight</th>\n",
       "      <th>strata</th>\n",
       "      <th>utl_exp_ppp17</th>\n",
       "      <th>male</th>\n",
       "      <th>hsize</th>\n",
       "      <th>num_children5</th>\n",
       "      <th>num_children10</th>\n",
       "      <th>num_children18</th>\n",
       "      <th>...</th>\n",
       "      <th>consumed4200</th>\n",
       "      <th>consumed4300</th>\n",
       "      <th>consumed4400</th>\n",
       "      <th>consumed4500</th>\n",
       "      <th>consumed4600</th>\n",
       "      <th>consumed4700</th>\n",
       "      <th>consumed4800</th>\n",
       "      <th>consumed4900</th>\n",
       "      <th>consumed5000</th>\n",
       "      <th>survey_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>4</td>\n",
       "      <td>594.80627</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100002</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>4</td>\n",
       "      <td>1676.27230</td>\n",
       "      <td>Female</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100003</td>\n",
       "      <td>1</td>\n",
       "      <td>375</td>\n",
       "      <td>4</td>\n",
       "      <td>506.93719</td>\n",
       "      <td>Male</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100004</td>\n",
       "      <td>1</td>\n",
       "      <td>375</td>\n",
       "      <td>4</td>\n",
       "      <td>824.61786</td>\n",
       "      <td>Male</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100005</td>\n",
       "      <td>1</td>\n",
       "      <td>525</td>\n",
       "      <td>4</td>\n",
       "      <td>351.47644</td>\n",
       "      <td>Male</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     hhid  com  weight  strata  utl_exp_ppp17    male  hsize  num_children5  \\\n",
       "0  100001    1      75       4      594.80627  Female      1              0   \n",
       "1  100002    1     150       4     1676.27230  Female      2              0   \n",
       "2  100003    1     375       4      506.93719    Male      5              0   \n",
       "3  100004    1     375       4      824.61786    Male      5              0   \n",
       "4  100005    1     525       4      351.47644    Male      7              1   \n",
       "\n",
       "   num_children10  num_children18  ...  consumed4200 consumed4300  \\\n",
       "0               0               0  ...           Yes           No   \n",
       "1               0               0  ...           Yes           No   \n",
       "2               0               2  ...           Yes          Yes   \n",
       "3               0               1  ...            No          Yes   \n",
       "4               0               0  ...           Yes           No   \n",
       "\n",
       "  consumed4400 consumed4500 consumed4600 consumed4700 consumed4800  \\\n",
       "0           No           No          Yes          Yes          Yes   \n",
       "1           No           No           No          Yes          Yes   \n",
       "2           No          Yes          Yes          Yes          Yes   \n",
       "3           No           No           No          Yes          Yes   \n",
       "4           No          Yes           No          Yes          Yes   \n",
       "\n",
       "  consumed4900 consumed5000  survey_id  \n",
       "0          Yes           No     100000  \n",
       "1           No           No     100000  \n",
       "2           No          Yes     100000  \n",
       "3           No           No     100000  \n",
       "4          Yes           No     100000  \n",
       "\n",
       "[5 rows x 88 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dimensions of training features\n",
    "print(\"Shape:\", train_data.shape)  # (rows, columns)\n",
    "print(\"Rows:\", len(train_data))\n",
    "print(\"Columns:\", len(train_data.columns))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b707ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (104234, 3)\n",
      "Rows: 104234\n",
      "Columns: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survey_id</th>\n",
       "      <th>hhid</th>\n",
       "      <th>cons_ppp17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>100001</td>\n",
       "      <td>25.258402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100000</td>\n",
       "      <td>100002</td>\n",
       "      <td>16.996706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100000</td>\n",
       "      <td>100003</td>\n",
       "      <td>13.671848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100000</td>\n",
       "      <td>100004</td>\n",
       "      <td>7.189475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100000</td>\n",
       "      <td>100005</td>\n",
       "      <td>12.308855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survey_id    hhid  cons_ppp17\n",
       "0     100000  100001   25.258402\n",
       "1     100000  100002   16.996706\n",
       "2     100000  100003   13.671848\n",
       "3     100000  100004    7.189475\n",
       "4     100000  100005   12.308855"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dimensions of training targets\n",
    "print(\"Shape:\", hh_data.shape)  # (rows, columns)\n",
    "print(\"Rows:\", len(hh_data))\n",
    "print(\"Columns:\", len(hh_data.columns))\n",
    "hh_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d74a7475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (3, 20)\n",
      "Rows: 3\n",
      "Columns: 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survey_id</th>\n",
       "      <th>pct_hh_below_3.17</th>\n",
       "      <th>pct_hh_below_3.94</th>\n",
       "      <th>pct_hh_below_4.60</th>\n",
       "      <th>pct_hh_below_5.26</th>\n",
       "      <th>pct_hh_below_5.88</th>\n",
       "      <th>pct_hh_below_6.47</th>\n",
       "      <th>pct_hh_below_7.06</th>\n",
       "      <th>pct_hh_below_7.70</th>\n",
       "      <th>pct_hh_below_8.40</th>\n",
       "      <th>pct_hh_below_9.13</th>\n",
       "      <th>pct_hh_below_9.87</th>\n",
       "      <th>pct_hh_below_10.70</th>\n",
       "      <th>pct_hh_below_11.62</th>\n",
       "      <th>pct_hh_below_12.69</th>\n",
       "      <th>pct_hh_below_14.03</th>\n",
       "      <th>pct_hh_below_15.64</th>\n",
       "      <th>pct_hh_below_17.76</th>\n",
       "      <th>pct_hh_below_20.99</th>\n",
       "      <th>pct_hh_below_27.37</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>0.067364</td>\n",
       "      <td>0.118927</td>\n",
       "      <td>0.169905</td>\n",
       "      <td>0.221865</td>\n",
       "      <td>0.271564</td>\n",
       "      <td>0.319585</td>\n",
       "      <td>0.366329</td>\n",
       "      <td>0.419816</td>\n",
       "      <td>0.471454</td>\n",
       "      <td>0.523798</td>\n",
       "      <td>0.574413</td>\n",
       "      <td>0.623091</td>\n",
       "      <td>0.671263</td>\n",
       "      <td>0.721329</td>\n",
       "      <td>0.773303</td>\n",
       "      <td>0.819770</td>\n",
       "      <td>0.865121</td>\n",
       "      <td>0.909075</td>\n",
       "      <td>0.954239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200000</td>\n",
       "      <td>0.059326</td>\n",
       "      <td>0.111560</td>\n",
       "      <td>0.159023</td>\n",
       "      <td>0.211754</td>\n",
       "      <td>0.263100</td>\n",
       "      <td>0.311758</td>\n",
       "      <td>0.356914</td>\n",
       "      <td>0.407631</td>\n",
       "      <td>0.463443</td>\n",
       "      <td>0.512931</td>\n",
       "      <td>0.559361</td>\n",
       "      <td>0.609337</td>\n",
       "      <td>0.659291</td>\n",
       "      <td>0.708043</td>\n",
       "      <td>0.760932</td>\n",
       "      <td>0.809045</td>\n",
       "      <td>0.860350</td>\n",
       "      <td>0.906385</td>\n",
       "      <td>0.952805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300000</td>\n",
       "      <td>0.049803</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.149502</td>\n",
       "      <td>0.200144</td>\n",
       "      <td>0.250192</td>\n",
       "      <td>0.300211</td>\n",
       "      <td>0.349596</td>\n",
       "      <td>0.399930</td>\n",
       "      <td>0.449845</td>\n",
       "      <td>0.499930</td>\n",
       "      <td>0.550082</td>\n",
       "      <td>0.599926</td>\n",
       "      <td>0.650088</td>\n",
       "      <td>0.699617</td>\n",
       "      <td>0.750341</td>\n",
       "      <td>0.800111</td>\n",
       "      <td>0.850081</td>\n",
       "      <td>0.899974</td>\n",
       "      <td>0.949988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survey_id  pct_hh_below_3.17  pct_hh_below_3.94  pct_hh_below_4.60  \\\n",
       "0     100000           0.067364           0.118927           0.169905   \n",
       "1     200000           0.059326           0.111560           0.159023   \n",
       "2     300000           0.049803           0.100381           0.149502   \n",
       "\n",
       "   pct_hh_below_5.26  pct_hh_below_5.88  pct_hh_below_6.47  pct_hh_below_7.06  \\\n",
       "0           0.221865           0.271564           0.319585           0.366329   \n",
       "1           0.211754           0.263100           0.311758           0.356914   \n",
       "2           0.200144           0.250192           0.300211           0.349596   \n",
       "\n",
       "   pct_hh_below_7.70  pct_hh_below_8.40  pct_hh_below_9.13  pct_hh_below_9.87  \\\n",
       "0           0.419816           0.471454           0.523798           0.574413   \n",
       "1           0.407631           0.463443           0.512931           0.559361   \n",
       "2           0.399930           0.449845           0.499930           0.550082   \n",
       "\n",
       "   pct_hh_below_10.70  pct_hh_below_11.62  pct_hh_below_12.69  \\\n",
       "0            0.623091            0.671263            0.721329   \n",
       "1            0.609337            0.659291            0.708043   \n",
       "2            0.599926            0.650088            0.699617   \n",
       "\n",
       "   pct_hh_below_14.03  pct_hh_below_15.64  pct_hh_below_17.76  \\\n",
       "0            0.773303            0.819770            0.865121   \n",
       "1            0.760932            0.809045            0.860350   \n",
       "2            0.750341            0.800111            0.850081   \n",
       "\n",
       "   pct_hh_below_20.99  pct_hh_below_27.37  \n",
       "0            0.909075            0.954239  \n",
       "1            0.906385            0.952805  \n",
       "2            0.899974            0.949988  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dimensions of poverty data\n",
    "print(\"Shape:\", pr_data.shape)  # (rows, columns)\n",
    "print(\"Rows:\", len(pr_data))\n",
    "print(\"Columns:\", len(pr_data.columns))\n",
    "pr_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109141c",
   "metadata": {},
   "source": [
    "## Proverty Rate Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92428d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs_data = hh_data[['survey_id','cons_ppp17']]\n",
    "w_data = train_data[['survey_id','weight']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "735732b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_poverty_thresholds(values, weights, surveys, pr_data):\n",
    "\n",
    "    # Define thresholds\n",
    "    thresholds = [3.17, 3.94, 4.60, 5.26, 5.88, 6.47, 7.06, 7.70, 8.40, 9.13, 9.87, 10.70, 11.62, 12.69, 14.03, 15.64, 17.76, 20.99, 27.37]\n",
    "    thresholds = np.array(thresholds, dtype=float)\n",
    "    pr = []\n",
    "\n",
    "    for s in range(surveys.shape[0]):\n",
    "        survey_values = values[values['survey_id'] == surveys[s]][['cons_ppp17']].to_numpy().flatten()\n",
    "        survey_weights = weights[weights['survey_id'] == surveys[s]][['weight']].to_numpy().flatten()\n",
    "\n",
    "        # Calculate percentage below each \n",
    "        percentages = [(survey_weights[survey_values < t].sum() / survey_weights.sum()) for t in thresholds]\n",
    "        pr.append(percentages)\n",
    "\n",
    "    df = pd.DataFrame([pr[0]], columns=pr_data.columns.tolist()[1:])\n",
    "    for d in range(1,len(pr)):\n",
    "        df = pd.concat([df, pd.DataFrame([pr[d]], columns=pr_data.columns.tolist()[1:])], ignore_index=True)\n",
    "    \n",
    "    df.index = surveys\n",
    "    df.index.name = \"survey_id\"\n",
    "\n",
    "    return df\n",
    "\n",
    "df = get_poverty_thresholds(hh_data, w_data, surveys=hh_data[\"survey_id\"].unique(), pr_data=pr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b802863b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pct_hh_below_3.17</th>\n",
       "      <th>pct_hh_below_3.94</th>\n",
       "      <th>pct_hh_below_4.60</th>\n",
       "      <th>pct_hh_below_5.26</th>\n",
       "      <th>pct_hh_below_5.88</th>\n",
       "      <th>pct_hh_below_6.47</th>\n",
       "      <th>pct_hh_below_7.06</th>\n",
       "      <th>pct_hh_below_7.70</th>\n",
       "      <th>pct_hh_below_8.40</th>\n",
       "      <th>pct_hh_below_9.13</th>\n",
       "      <th>pct_hh_below_9.87</th>\n",
       "      <th>pct_hh_below_10.70</th>\n",
       "      <th>pct_hh_below_11.62</th>\n",
       "      <th>pct_hh_below_12.69</th>\n",
       "      <th>pct_hh_below_14.03</th>\n",
       "      <th>pct_hh_below_15.64</th>\n",
       "      <th>pct_hh_below_17.76</th>\n",
       "      <th>pct_hh_below_20.99</th>\n",
       "      <th>pct_hh_below_27.37</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survey_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>0.067364</td>\n",
       "      <td>0.118927</td>\n",
       "      <td>0.169905</td>\n",
       "      <td>0.221865</td>\n",
       "      <td>0.271564</td>\n",
       "      <td>0.319585</td>\n",
       "      <td>0.366329</td>\n",
       "      <td>0.419816</td>\n",
       "      <td>0.471454</td>\n",
       "      <td>0.523798</td>\n",
       "      <td>0.574413</td>\n",
       "      <td>0.623091</td>\n",
       "      <td>0.671263</td>\n",
       "      <td>0.721329</td>\n",
       "      <td>0.773303</td>\n",
       "      <td>0.819770</td>\n",
       "      <td>0.865121</td>\n",
       "      <td>0.909075</td>\n",
       "      <td>0.954239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200000</th>\n",
       "      <td>0.059326</td>\n",
       "      <td>0.111560</td>\n",
       "      <td>0.159023</td>\n",
       "      <td>0.211754</td>\n",
       "      <td>0.263100</td>\n",
       "      <td>0.311758</td>\n",
       "      <td>0.356914</td>\n",
       "      <td>0.407631</td>\n",
       "      <td>0.463443</td>\n",
       "      <td>0.512931</td>\n",
       "      <td>0.559361</td>\n",
       "      <td>0.609337</td>\n",
       "      <td>0.659291</td>\n",
       "      <td>0.708043</td>\n",
       "      <td>0.760932</td>\n",
       "      <td>0.809045</td>\n",
       "      <td>0.860350</td>\n",
       "      <td>0.906385</td>\n",
       "      <td>0.952805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300000</th>\n",
       "      <td>0.049803</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.149502</td>\n",
       "      <td>0.200144</td>\n",
       "      <td>0.250192</td>\n",
       "      <td>0.300211</td>\n",
       "      <td>0.349596</td>\n",
       "      <td>0.399930</td>\n",
       "      <td>0.449845</td>\n",
       "      <td>0.499930</td>\n",
       "      <td>0.550082</td>\n",
       "      <td>0.599926</td>\n",
       "      <td>0.650088</td>\n",
       "      <td>0.699617</td>\n",
       "      <td>0.750341</td>\n",
       "      <td>0.800111</td>\n",
       "      <td>0.850081</td>\n",
       "      <td>0.899974</td>\n",
       "      <td>0.949988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pct_hh_below_3.17  pct_hh_below_3.94  pct_hh_below_4.60  \\\n",
       "survey_id                                                            \n",
       "100000              0.067364           0.118927           0.169905   \n",
       "200000              0.059326           0.111560           0.159023   \n",
       "300000              0.049803           0.100381           0.149502   \n",
       "\n",
       "           pct_hh_below_5.26  pct_hh_below_5.88  pct_hh_below_6.47  \\\n",
       "survey_id                                                            \n",
       "100000              0.221865           0.271564           0.319585   \n",
       "200000              0.211754           0.263100           0.311758   \n",
       "300000              0.200144           0.250192           0.300211   \n",
       "\n",
       "           pct_hh_below_7.06  pct_hh_below_7.70  pct_hh_below_8.40  \\\n",
       "survey_id                                                            \n",
       "100000              0.366329           0.419816           0.471454   \n",
       "200000              0.356914           0.407631           0.463443   \n",
       "300000              0.349596           0.399930           0.449845   \n",
       "\n",
       "           pct_hh_below_9.13  pct_hh_below_9.87  pct_hh_below_10.70  \\\n",
       "survey_id                                                             \n",
       "100000              0.523798           0.574413            0.623091   \n",
       "200000              0.512931           0.559361            0.609337   \n",
       "300000              0.499930           0.550082            0.599926   \n",
       "\n",
       "           pct_hh_below_11.62  pct_hh_below_12.69  pct_hh_below_14.03  \\\n",
       "survey_id                                                               \n",
       "100000               0.671263            0.721329            0.773303   \n",
       "200000               0.659291            0.708043            0.760932   \n",
       "300000               0.650088            0.699617            0.750341   \n",
       "\n",
       "           pct_hh_below_15.64  pct_hh_below_17.76  pct_hh_below_20.99  \\\n",
       "survey_id                                                               \n",
       "100000               0.819770            0.865121            0.909075   \n",
       "200000               0.809045            0.860350            0.906385   \n",
       "300000               0.800111            0.850081            0.899974   \n",
       "\n",
       "           pct_hh_below_27.37  \n",
       "survey_id                      \n",
       "100000               0.954239  \n",
       "200000               0.952805  \n",
       "300000               0.949988  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aba92a5",
   "metadata": {},
   "source": [
    "## Check NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e5de454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of missing values in each column\n",
    "nan_percent = train_data.isna().mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd47a057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables with more than 10% missing values:\n",
      "sector1d    13.555078\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Remove variables with more than 10% missing values\n",
    "missing_percent = nan_percent[nan_percent > 10]\n",
    "print(\"Variables with more than 10% missing values:\")\n",
    "print(missing_percent)\n",
    "\n",
    "# Remove these variables from the dataset\n",
    "train_data = train_data.drop(columns=missing_percent.index.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecea6710",
   "metadata": {},
   "source": [
    "## Remove Variables Not Used As Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee5eb05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns from train_data\n",
    "weights = train_data['weight']\n",
    "survey = train_data['survey_id']\n",
    "columns_to_remove = ['hhid', 'com', 'strata', 'weight']\n",
    "train_data = train_data.drop(columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6655b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utl_exp_ppp17</th>\n",
       "      <th>male</th>\n",
       "      <th>hsize</th>\n",
       "      <th>num_children5</th>\n",
       "      <th>num_children10</th>\n",
       "      <th>num_children18</th>\n",
       "      <th>age</th>\n",
       "      <th>owner</th>\n",
       "      <th>water</th>\n",
       "      <th>toilet</th>\n",
       "      <th>...</th>\n",
       "      <th>consumed4200</th>\n",
       "      <th>consumed4300</th>\n",
       "      <th>consumed4400</th>\n",
       "      <th>consumed4500</th>\n",
       "      <th>consumed4600</th>\n",
       "      <th>consumed4700</th>\n",
       "      <th>consumed4800</th>\n",
       "      <th>consumed4900</th>\n",
       "      <th>consumed5000</th>\n",
       "      <th>survey_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>594.80627</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>Owner</td>\n",
       "      <td>Access</td>\n",
       "      <td>Access</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1676.27230</td>\n",
       "      <td>Female</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>Owner</td>\n",
       "      <td>Access</td>\n",
       "      <td>Access</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>506.93719</td>\n",
       "      <td>Male</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>Owner</td>\n",
       "      <td>Access</td>\n",
       "      <td>Access</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>824.61786</td>\n",
       "      <td>Male</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>Not owner</td>\n",
       "      <td>Access</td>\n",
       "      <td>Access</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>351.47644</td>\n",
       "      <td>Male</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>Owner</td>\n",
       "      <td>Access</td>\n",
       "      <td>Access</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   utl_exp_ppp17    male  hsize  num_children5  num_children10  \\\n",
       "0      594.80627  Female      1              0               0   \n",
       "1     1676.27230  Female      2              0               0   \n",
       "2      506.93719    Male      5              0               0   \n",
       "3      824.61786    Male      5              0               0   \n",
       "4      351.47644    Male      7              1               0   \n",
       "\n",
       "   num_children18  age      owner   water  toilet  ... consumed4200  \\\n",
       "0               0   75      Owner  Access  Access  ...          Yes   \n",
       "1               0   61      Owner  Access  Access  ...          Yes   \n",
       "2               2   49      Owner  Access  Access  ...          Yes   \n",
       "3               1   58  Not owner  Access  Access  ...           No   \n",
       "4               0   57      Owner  Access  Access  ...          Yes   \n",
       "\n",
       "  consumed4300 consumed4400 consumed4500 consumed4600  consumed4700  \\\n",
       "0           No           No           No          Yes           Yes   \n",
       "1           No           No           No           No           Yes   \n",
       "2          Yes           No          Yes          Yes           Yes   \n",
       "3          Yes           No           No           No           Yes   \n",
       "4           No           No          Yes           No           Yes   \n",
       "\n",
       "   consumed4800  consumed4900 consumed5000  survey_id  \n",
       "0           Yes           Yes           No     100000  \n",
       "1           Yes            No           No     100000  \n",
       "2           Yes            No          Yes     100000  \n",
       "3           Yes            No           No     100000  \n",
       "4           Yes           Yes           No     100000  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b735841",
   "metadata": {},
   "source": [
    "## Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03f0871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = hh_data[\"cons_ppp17\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d22b8b4",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393361c6",
   "metadata": {},
   "source": [
    "### Infer Feature Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d8be4469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: ['utl_exp_ppp17', 'hsize', 'num_children5', 'num_children10', 'num_children18', 'age', 'num_adult_female', 'num_adult_male', 'num_elderly', 'sworkershh', 'share_secondary', 'sfworkershh', 'region1', 'region2', 'region3', 'region4', 'region5', 'region6', 'region7']\n",
      "Categorical features: ['male', 'owner', 'water', 'toilet', 'sewer', 'elect', 'water_source', 'sanitation_source', 'dweltyp', 'employed', 'educ_max', 'any_nonagric', 'urban', 'consumed100', 'consumed200', 'consumed300', 'consumed400', 'consumed500', 'consumed600', 'consumed700', 'consumed800', 'consumed900', 'consumed1000', 'consumed1100', 'consumed1200', 'consumed1300', 'consumed1400', 'consumed1500', 'consumed1600', 'consumed1700', 'consumed1800', 'consumed1900', 'consumed2000', 'consumed2100', 'consumed2200', 'consumed2300', 'consumed2400', 'consumed2500', 'consumed2600', 'consumed2700', 'consumed2800', 'consumed2900', 'consumed3000', 'consumed3100', 'consumed3200', 'consumed3300', 'consumed3400', 'consumed3500', 'consumed3600', 'consumed3700', 'consumed3800', 'consumed3900', 'consumed4000', 'consumed4100', 'consumed4200', 'consumed4300', 'consumed4400', 'consumed4500', 'consumed4600', 'consumed4700', 'consumed4800', 'consumed4900', 'consumed5000', 'survey_id']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def infer_feature_types(df, year_col=\"year\"):\n",
    "    feature_cols = [c for c in df.columns]\n",
    "    # Simple heuristic: non-numeric or low-cardinality numeric treated as categorical (including year)\n",
    "    numeric_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    categorical_cols = [c for c in feature_cols if c not in numeric_cols]\n",
    "\n",
    "    # Ensure year is treated as categorical (common for pooled models)\n",
    "    if year_col in numeric_cols:\n",
    "        numeric_cols.remove(year_col)\n",
    "    if year_col not in categorical_cols:\n",
    "        categorical_cols.append(year_col)\n",
    "    return numeric_cols, categorical_cols\n",
    "\n",
    "numeric_features, categorical_features = infer_feature_types(train_data, year_col=\"survey_id\")\n",
    "print(\"Numeric features:\", numeric_features)\n",
    "print(\"Categorical features:\", categorical_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af142a",
   "metadata": {},
   "source": [
    "    ### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc01b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Preprocess: impute NaNs, then encode/scale as needed\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Median imputation for numeric, then (optional) scale\n",
    "        (\"num\", Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "        ]), numeric_features),\n",
    "\n",
    "        # Most-frequent imputation for categorical, then one-hot encode\n",
    "        (\"cat\", Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]), categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f41b9d",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b2ccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(random_state=42,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc94a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pooled_regression(df, target_col=\"y\", year_col=\"year\",\n",
    "                      model=None, test_size=0.2, random_state=42):\n",
    "    # Default model: Random Forest (robust, handles nonlinearity)\n",
    "    if model is None:\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=400,\n",
    "            max_depth=None,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    df_obs, df_mis = split_observed_missing(df, target_col)\n",
    "    numeric_cols, categorical_cols = infer_feature_types(df, target_col, year_col)\n",
    "\n",
    "    # Preprocess: scale numeric (optional), one-hot categorical (including 'year')\n",
    "    preproc = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", StandardScaler(with_mean=True, with_std=True), numeric_cols),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline(steps=[(\"prep\", preproc), (\"model\", model)])\n",
    "\n",
    "    # Train/validation split only on observed rows\n",
    "    X = df_obs.drop(columns=[target_col])\n",
    "    y = df_obs[target_col]\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "\n",
    "    # Evaluate (on observed years)\n",
    "    y_pred_tr = pipe.predict(X_tr)\n",
    "    y_pred_te = pipe.predict(X_te)\n",
    "\n",
    "    metrics = {\n",
    "        \"train_R2\": r2_score(y_tr, y_pred_tr),\n",
    "        \"test_R2\": r2_score(y_te, y_pred_te),\n",
    "        \"train_RMSE\": mean_squared_error(y_tr, y_pred_tr, squared=False),\n",
    "        \"test_RMSE\": mean_squared_error(y_te, y_pred_te, squared=False),\n",
    "    }\n",
    "\n",
    "    # Predict missing years (impute)\n",
    "    if not df_mis.empty:\n",
    "        X_missing = df_mis.drop(columns=[target_col])\n",
    "        y_imputed = pipe.predict(X_missing)\n",
    "        df_imputed = df_mis.copy()\n",
    "        df_imputed[target_col] = y_imputed\n",
    "        # Combine back with observed to get a “completed” dataset\n",
    "        df_completed = pd.concat([df_obs, df_imputed], axis=0).sort_index()\n",
    "    else:\n",
    "        df_completed = df.copy()\n",
    "\n",
    "    return pipe, metrics, df_completed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8db23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: df has columns ['year', 'age', 'education', 'region', 'y']\n",
    "# where 'y' is continuous (e.g., income) and is missing in some years\n",
    "model, metrics, df_completed = pooled_regression(df, target_col=\"y\", year_col=\"year\")\n",
    "print(metrics)\n",
    "# df_completed now contains imputed y for the years where it was missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9061763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "def train_and_impute_across_years(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = \"cons_pc\",\n",
    "    year_col: str = \"year\",\n",
    "    train_years=None,\n",
    "    test_years=None,\n",
    "    model=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a pooled model using multiple years and predict the target for test years.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Tidy data with columns: `year_col`, `target_col`, and feature columns.\n",
    "    target_col : str\n",
    "        Name of the target variable (e.g., per-capita consumption).\n",
    "    year_col : str\n",
    "        Name of the year column.\n",
    "    train_years : list or array-like\n",
    "        Years to use for training. If None, uses all years except `test_years`.\n",
    "    test_years : list or array-like\n",
    "        Years to predict. If None, uses the most recent year as test.\n",
    "    model : sklearn regressor or None\n",
    "        If None, uses RandomForestRegressor with reasonable defaults.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pipe : fitted sklearn Pipeline\n",
    "    metrics_by_year : dict\n",
    "        Dictionary of metrics (R2, RMSE) computed on test years where target is available.\n",
    "    df_with_preds : DataFrame\n",
    "        Input DataFrame with a new column f'{target_col}_pred' for rows in test_years.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Decide train/test years if not provided\n",
    "    all_years = sorted(pd.unique(df[year_col]))\n",
    "    if test_years is None:\n",
    "        test_years = [max(all_years)]  # last year as default test\n",
    "    if train_years is None:\n",
    "        train_years = [y for y in all_years if y not in set(test_years)]\n",
    "\n",
    "    # Split train/test rows\n",
    "    is_train = df[year_col].isin(train_years) & df[target_col].notna()\n",
    "    is_test  = df[year_col].isin(test_years)\n",
    "\n",
    "    if is_train.sum() == 0:\n",
    "        raise ValueError(\"No training rows found. Check `train_years` and presence of target.\")\n",
    "    if is_test.sum() == 0:\n",
    "        raise ValueError(\"No test rows found. Check `test_years`.\")\n",
    "\n",
    "    # Feature columns: everything except target\n",
    "    feature_cols = [c for c in df.columns if c != target_col]\n",
    "\n",
    "    # Infer numeric/categorical features\n",
    "    numeric_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(df[c]) and c != year_col]\n",
    "    categorical_cols = [c for c in feature_cols if c not in numeric_cols]\n",
    "\n",
    "    # Ensure year is treated as categorical (pooled‑years approach)\n",
    "    if year_col in numeric_cols:\n",
    "        numeric_cols.remove(year_col)\n",
    "    if year_col not in categorical_cols:\n",
    "        categorical_cols.append(year_col)\n",
    "\n",
    "    preproc = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", StandardScaler(), numeric_cols),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "\n",
    "    if model is None:\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=400,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    pipe = Pipeline(steps=[(\"prep\", preproc), (\"model\", model)])\n",
    "\n",
    "    # Fit on training years (only rows with target)\n",
    "    X_train = df.loc=is_train, feature_cols]\n",
    "    y_train = df.loc=is_train, target_col]\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test years\n",
    "    X_test = df.loc=is_test, feature_cols]\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    pred_col = f\"{target_col}_pred\"\n",
    "    df.loc[is_test, pred_col] = y_pred\n",
    "\n",
    "    # Evaluate per test year where the true target is available (optional)\n",
    "    metrics_by_year = {}\n",
    "    for y in test_years:\n",
    "        mask = (df[year_col] == y) & df[target_col].notna() & df[pred_col].notna()\n",
    "        if mask.sum() > 1:  # need at least 2 points for R2\n",
    "            y_true = df.loc[mask, target_col]\n",
    "            y_hat  = df.loc[mask, pred_col]\n",
    "            r2 = r2_score(y_true, y_hat)\n",
    "            rmse = mean_squared_error(y_true, y_hat, squared=False)\n",
    "            metrics_by_year[y] = {\"R2\": r2, \"RMSE\": rmse}\n",
    "        else:\n",
    "            # No ground truth available in that year (e.g., true imputation year)\n",
    "            metrics_by_year[y] = {\"R2\": np.nan, \"RMSE\": np.nan}\n",
    "\n",
    "    return pipe, metrics_by_year, df\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Example usage\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: df has columns ['year','region','urban','hh_size','educ_head','cons_pc', ...]\n",
    "    # Replace with your actual DataFrame.\n",
    "    # df = pd.read_csv(\"your_survey_data.csv\")\n",
    "    # For demonstration, create a small dummy dataset:\n",
    "    rng = np.random.default_rng(0)\n",
    "    n = 1000\n",
    "    df_demo = pd.DataFrame({\n",
    "        \"year\": rng.choice([2017, 2018, 2019, 2020, 2021], size=n),\n",
    "        \"region\": rng.choice([\"N\", \"S\", \"E\", \"W\"], size=n),\n",
    "        \"urban\": rng.choice([0, 1], size=n),\n",
    "        \"hh_size\": rng.integers(1, 8, size=n),\n",
    "        \"educ_head\": rng.choice([\"none\", \"primary\", \"secondary\", \"tertiary\"], size=n),\n",
    "        \"cons_pc\": rng.normal(1000, 250, size=n).clip(200, None),\n",
    "    })\n",
    "\n",
    "    # Suppose we want to train on 2017–2020 and predict 2021 (test/imputation)\n",
    "    pipe, metrics, df_out = train_and_impute_across_years(\n",
    "        df_demo,\n",
    "        target_col=\"cons_pc\",\n",
    "        year_col=\"year\",\n",
    "        train_years=[2017, 2018, 2019, 2020],\n",
    "        test_years=[2021]\n",
    "    )\n",
    "\n",
    "    print(\"Evaluation on test years (if ground truth available):\")\n",
    "    print(metrics)\n",
    "\n",
    "    # df_out now contains 'cons_pc_pred' for rows in test years\n",
    "    print(df_out.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
